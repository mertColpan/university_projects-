from transformers import AutoTokenizer, AutoModel
import torch
from embedding_utils import create_vectors, create_input_vectors
from data_utils import read_excel, select_n_data
from similarity_evaluation import find_close_answer_with_model_3, find_close_answer_with_model_1_2, calculate_correct_prediction 
from model_traning import train_model_with_random_forest, train_model_with_logistic_regression
from sentence_transformers import SentenceTransformer
from collections import defaultdict, Counter
import test
import argparse
from train_all import train_all

def main():
    # Cihaz ayarÄ± (GPU varsa kullanÄ±lÄ±r, yoksa CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Komut satÄ±rÄ± argÃ¼manlarÄ± tanÄ±mlanÄ±yor
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, required=True, help="Model adÄ± (Ã¶rnek: E5, Jina)")
    parser.add_argument("--sample_size", type=int, default=1000, help="KaÃ§ sample kullanÄ±lacak")
    parser.add_argument("--task", type=int, choices=[1, 2, 3, 4], default=1, help="GÃ¶rev numarasÄ± (1 , 2, 3, 4)")
    args = parser.parse_args()

    # ArgÃ¼manlar deÄŸiÅŸkenlere atanÄ±yor
    model_name = args.model_name
    sample_size = args.sample_size
    task = args.task

    # Verisetinin okunmasÄ±
    datapath = './data/data.xlsx'
    dataframe = read_excel(datapath)
    
    # Belirli sayÄ±da soru, cevap ve etiket seÃ§iliyor
    questions, gpt_answer, deepseek_answer, label = select_n_data(dataframe, sample_size)

    # GÃ¶rev 1: Cevap benzerlik deÄŸerlendirmesi yapÄ±lÄ±yor
    if(task == 1):
        if(model_name == 'E5'):
            # E5 model yÃ¼kleniyor
            job = 'Given a web search query, retrieve relevant passages that answer the query'
            model_path = 'intfloat/multilingual-e5-large-instruct'
            model = SentenceTransformer(model_path, device=device)
            model.to(device)

            # GPT ve Deepseek cevaplarÄ± iÃ§in skorlar hesaplanÄ±yor
            gpt_score, gpt_top1, gpt_top5, _ , _ = find_close_answer_with_model_1_2(job, questions, gpt_answer, sample_size, model)
            deepseek_score, deepseek_top1, deepseek_top5, _ , _ = find_close_answer_with_model_1_2(job, questions, deepseek_answer, sample_size, model)

        elif(model_name == 'cosmosE5'):
            # TÃ¼rkÃ§e E5 modeli yÃ¼kleniyor
            job = 'Given a Turkish search query, retrieve relevant passages written in Turkish that best answer the query'
            model_path = 'ytu-ce-cosmos/turkish-e5-large'
            model = SentenceTransformer(model_path, device=device)
            model.to(device)

            # GPT ve Deepseek cevaplarÄ± iÃ§in skorlar hesaplanÄ±yor
            gpt_score, gpt_top1, gpt_top5, _ , _ = find_close_answer_with_model_1_2(job, questions, gpt_answer, sample_size, model)
            deepseek_score, deepseek_top1, deepseek_top5, _ , _ = find_close_answer_with_model_1_2(job, questions, deepseek_answer, sample_size, model)
            
        else:
            # Jina embeddings modeli yÃ¼kleniyor
            model_path = "./jina-embeddings-v3"
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # GPT ve Deepseek cevaplarÄ± iÃ§in skorlar hesaplanÄ±yor
            gpt_score, gpt_top1, gpt_top5, _, _ = find_close_answer_with_model_3(questions, gpt_answer, tokenizer, model_path, device, sample_size)
            deepseek_score, deepseek_top1, deepseek_top5, _ , _ = find_close_answer_with_model_3(questions, deepseek_answer, tokenizer, model_path, device, sample_size)

        # Elde edilen skorlarla doÄŸru tahmin oranÄ± hesaplanÄ±yor
        calculate_correct_prediction(gpt_score, gpt_top1, gpt_top5, deepseek_score, deepseek_top1, deepseek_top5, label, sample_size)
    
    # GÃ¶rev 2: CÃ¼mle vektÃ¶rlerinden input feature oluÅŸturulup logistic regression ile model eÄŸitiliyor
    elif(task == 2):
        if(model_name == 'E5'):
            # E5 modeli yÃ¼kleniyor
            model_path = 'intfloat/multilingual-e5-large-instruct'
            model = SentenceTransformer(model_path, device=device)
            model.to(device)
        elif(model_name == 'cosmosE5'):
            # TÃ¼rkÃ§e E5 modeli yÃ¼kleniyor
            model_path = 'ytu-ce-cosmos/turkish-e5-large'
            model = SentenceTransformer(model_path, device=device)
            model.to(device)
        else:
            # Jina embeddings modeli yÃ¼kleniyor
            model_path = "./jina-embeddings-v3"
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
            model.to(device)

        # Soru, GPT cevabÄ± ve Deepseek cevabÄ±ndan vektÃ¶rler oluÅŸturuluyor
        s, g, d = create_vectors(gpt_answer, deepseek_answer, questions, model)

        # VektÃ¶rlerden input feature seti oluÅŸturuluyor
        input_feature = create_input_vectors(s, g, d)

        # Logistic regression modeli ile eÄŸitim yapÄ±lÄ±yor
        train_model_with_logistic_regression(input_feature, label)
    
    # GÃ¶rev 3: Test fonksiyonu Ã§aÄŸÄ±rÄ±lÄ±yor
    if(task == 3):
        test.test(sample_size, dataframe)
    
    # GÃ¶rev 4: Ã‡oklu denemelerle en iyi vektÃ¶r kombinasyonlarÄ±nÄ± bulma
    if(task == 4):
        # Model yÃ¼kleniyor
        model_path = 'ytu-ce-cosmos/turkish-e5-large'
        model = SentenceTransformer(model_path, device=device)
        model.to(device)

        num_runs = 2    # KaÃ§ tekrar yapÄ±lacak
        vector_counter = Counter()  # VektÃ¶r kombinasyonlarÄ±nÄ±n sayÄ±sÄ±nÄ± tutacak

        # Belirlenen sayÄ±da tekrar yapÄ±lÄ±r
        for i in range(num_runs):
            print(f"ğŸ” {i+1}. tekrar")
            
            # Her seferinde farklÄ± sample seÃ§iliyor
            questions, gpt_answer, deepseek_answer, labels = select_n_data(dataframe, sample_size)
            s_embed, g_embed, d_embed = create_vectors(gpt_answer, deepseek_answer, questions, model)
            
            # EÄŸitim ve en iyi kombinasyonun bulunmasÄ±
            best_combo = train_all(s_embed, g_embed, d_embed, labels) 
            if best_combo:
                vector_counter.update(best_combo)  # SayacÄ± gÃ¼ncelle
        
        # SonuÃ§lar yazdÄ±rÄ±lÄ±yor
        print("\nğŸ 50 tekrar sonunda en Ã§ok kullanÄ±lan vektÃ¶rler:")
        for k, v in vector_counter.most_common():
            print(f"{k}: {v} kez")

# ProgramÄ±n baÅŸlangÄ±Ã§ noktasÄ±
if __name__ == '__main__':
    main()
